services:
################### Kafka ###################
  kafka_broker:
    image: confluentinc/cp-kafka:7.4.1
    hostname: broker
    container_name: kafka-broker
    ports:
     - 29092:29092
    environment:
    # KRAFT mode
      KAFKA_BROKER_ID: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_PROCESS_ROLES: broker,controller # role of this server
      KAFKA_NODE_ID: 1 # unique id for this server
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@broker:29093 # quorum voters
      ####
      KAFKA_LISTENERS: PLAINTEXT://broker:9092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:29092 # listeners
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,PLAINTEXT_HOST://localhost:29092 # metadata listeners
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT # key-value pair for security protocol to use per listener name
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT # listener to use for inter-broker communication
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER # listeners used by controller
      ####
      #KAFKA_LOG_DIRS: /tmp/kraft-combined-logs
      CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk

    # Stackoverflow https://stackoverflow.com/questions/64865361/docker-compose-create-kafka-topics
    # Use plaintexthost(KAFKA_INTER_BROKER_LISTENER_NAME) port 
  kafka_init:
    image: confluentinc/cp-kafka:7.4.1
    container_name: kafka-init
    depends_on:
      - kafka_broker
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      # blocks until kafka is reachable
      kafka-topics --bootstrap-server broker:9092 --list

      echo -e 'Creating kafka topics'
      kafka-topics --bootstrap-server broker:9092 --create --if-not-exists --topic ${TEMP_TOPIC} --replication-factor 1 --partitions 1
      kafka-topics --bootstrap-server broker:9092 --create --if-not-exists --topic ${HUM_TOPIC} --replication-factor 1 --partitions 1

      echo -e 'Successfully created the following topics:'
      kafka-topics --bootstrap-server broker:9092 --list
      "

  kafka_connect:
    build:
      context: .
      dockerfile: connect.Dockerfile
    hostname: connect
    container_name: kafka-connect
    depends_on:
      - kafka_broker
    environment:
      KAFKA_JMX_PORT: 35000
      KAFKA_JMX_HOSTNAME: localhost
      CONNECT_BOOTSTRAP_SERVERS: "kafka_broker:9092"
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: connect-cluster-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_ZOOKEEPER_CONNECT: "zookeeper:2181"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
      CONNECT_CONNECTIONS_MAX_IDLE_MS: 180000
      CONNECT_METADATA_MAX_AGE_MS: 180000
      CONNECT_AUTO_CREATE_TOPICS_ENABLE: "true"
      CONNECT_KEY_CONVERTER: 'org.apache.kafka.connect.storage.StringConverter'
      CONNECT_VALUE_CONVERTER: 'org.apache.kafka.connect.storage.StringConverter'
    volumes:
      #- ./kafka_connect/mongo-kafka-connect-1.12.0-all.jar:/usr/share/confluent-hub-components/mongo-kafka-connect-1.12.0-all.jar:ro
      - ./kafka_connect/mongo-sink-connector.json:/mongo-sink-connector.json
    working_dir: /
    # command: 
    #   - curl -X POST -H "Content-Type: application/json" --data @mongo-sink-connector.json http://kafka_connect:8083/connectors -w "\n"


    # bin/connect-standalone config/connect.properties config/mongodb-sink-temperature.properties

  # Sensors(H:01 T:234)
  # kafka-producer-0 :
  #   image: producer:latest
  #   container_name: kafka-p0 # alpine 
  #   working_dir: /app
  #   volumes:  # Mount folder with all sensor data
  #     - ./data:/app/data  
  #   depends_on:
  #     - broker
  #   command: /bin/sh -c "java -cp build/libs/kafka-producer-sensor-0.0.1.jar sensors.SensorProducerApplication RTproducer.properties data/sensor_0.json humidity-topic"
  # kafka-producer-1 :
  #   image: producer:latest
  #   container_name: kafka-p1 # alpine 
  #   working_dir: /app
  #   volumes:  # Mount folder with all sensor data
  #     - ./data:/app/data  
  #   depends_on:
  #     - broker
  #   command: /bin/sh -c "java -cp build/libs/kafka-producer-sensor-0.0.1.jar sensors.SensorProducerApplication RTproducer.properties data/sensor_1.json humidity-topic"
  # kafka-producer-2 :
  #   image: producer:latest
  #   container_name: kafka-p2 # alpine 
  #   working_dir: /app
  #   volumes:  # Mount folder with all sensor data
  #     - ./data:/app/data  
  #   depends_on:
  #     - broker
  #   command: /bin/sh -c "java -cp build/libs/kafka-producer-sensor-0.0.1.jar sensors.SensorProducerApplication RTproducer.properties data/sensor_2.json temperature-topic"
  # kafka-producer-3 :
  #   image: producer:latest
  #   container_name: kafka-p3 # alpine 
  #   working_dir: /app
  #   volumes:  # Mount folder with all sensor data
  #     - ./data:/app/data  
  #   depends_on:
  #     - broker
  #   command: /bin/sh -c "java -cp build/libs/kafka-producer-sensor-0.0.1.jar sensors.SensorProducerApplication RTproducer.properties data/sensor_3.json temperature-topic"
  kafka_producer_4 :
    image: producer:latest
    container_name: kafka-p4 # alpine 
    working_dir: /app
    volumes:
      - ./data:/app/data  
    depends_on:
      - kafka_broker
      - mongodb
      - kafka_connect
    command: /bin/sh -c "java -cp build/libs/kafka-producer-sensor-0.0.1.jar sensors.SensorProducerApplication RTproducer.properties data/sensor_4.json temperature-topic"

#["java", "-cp", "/app/build/libs/kafka-producer-sensor-0.0.1.jar", "sensors.SensorProducerApplication ", "RTproducer.properties", "data/sensor_0.json"] 
#- java -cp build/libs/kafka-producer-sensor-0.0.1.jar sensors.SensorProducerApplication RTproducer.properties data/sensor_0.json
# ["sleep", "infinite"] 
      
# kafka-connect: confluentinc/cp-kafka-connect
# confluent docker config reference kafka example

################### Flink standalone application mode ###################
  # flink_jobmanager:
  #   image: pyflink_alert:latest
  #   container_name: flink-jobmanager
  #   depends_on:
  #     - kafka_broker
  #   ports:
  #     - "8082:8081"
  #   #entrypoint: ['/bin/sh', '-c']
  #   command: >
  #     jobmanager
  #   volumes:
  #    - ./flink/code:/opt/flink/code
  #   environment:
  #     - |
  #       FLINK_PROPERTIES=
  #       jobmanager.rpc.address: jobmanager
  #       parallelism.default: 2     
      
  # flink_taskmanager:
  #   image: pyflink_alert:latest
  #   container_name: flink-taskmanager
  #   depends_on:
  #     - flink_jobmanager
  #   command: >
  #       taskmanager
  #   scale: 1
  #   volumes:
  #     - ./flink/code:/opt/flink/code
  #   environment:
  #     - |
  #       FLINK_PROPERTIES=
  #       jobmanager.rpc.address: jobmanager
  #       taskmanager.numberOfTaskSlots: 2 
  #       parallelism.default: 2 

################### Mongo ###################
### Config variables https://hub.docker.com/r/bitnami/mongodb
  mongodb:
    image: mongo:7.0.10-rc0-jammy
    container_name: mongodb
    ports:
      - 12345:27017
    environment:
      - MONGO_INITDB_ROOT_USERNAME=root
      - MONGO_INITDB_ROOT_PASSWORD=root
      - MONGODB_INITDB_DATABASE=root
    volumes:
      - ./mongo/mongo-init.js:/docker-entrypoint-initdb.d/mongo-init.js:ro 

